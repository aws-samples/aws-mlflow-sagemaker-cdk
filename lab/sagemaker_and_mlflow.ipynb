{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Scikit-Learn model in SageMaker and track with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "The main objective of this notebook is to show how you can integrate Amazon SageMaker and MLFlow.\n",
    "\n",
    "## Pre-Requisites\n",
    "\n",
    "In order to run successfully this notebook, you must have prepared the infrastructure using CDK, which setups up for you the MLFlow server in an isolated VPC. When running this example in the SageMaker Notebook instance provisioned via CDK, the following environmental variable is also automatically set for you via notebook lifecycle policies:\n",
    "\n",
    "* `MLFLOWSERVER` - the URI of the MLFlow server we will use for tracking purposes. In our case, this corresponds to the `HTTP API Gateway` endpoint that exposes our MLFlow server reacheable via a `PrivateLink`\n",
    "\n",
    "* `MLFLOW_SECRET_NAME` - the name of the secret in Amazon SecretsManager \n",
    "\n",
    "* `MLFLOW_KEY` - key within the secret in the Amazon SecretsManager corresponding to the token for the authentication \n",
    "\n",
    "## The Machine Learning Problem\n",
    "\n",
    "In this example, we will solve a regression problem which aims to answer the question: \"what is the expected price of a house in the California area?\". The target variable is the house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "## Install required and/or update libraries\n",
    "\n",
    "At the time of writing, the `sagemaker` SDK version tested is `2.63.1`, while the MLFlow SDK library used is the one corresponding to our MLFlow server version, i.e., `1.18.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade sagemaker==2.63.1\n",
    "!pip install -q --upgrade mlflow==1.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the notebook instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/using-identity-based-policies.html) for more details on creating these.  Note, if a role not associated with the current notebook instance, or more than one role is required for training and/or hosting, please replace `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n",
    "- The tracking URI where the MLFlow server runs\n",
    "- The experiment name as the logical entity to keep our tests grouped and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "\n",
    "## SageMaker and SKlearn libraries\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tuner import IntegerParameter, HyperparameterTuner\n",
    "\n",
    "## SKLearn libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## MLFlow libraries\n",
    "import mlflow\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "import mlflow.sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "account = role.split(\"::\")[1].split(\":\")[0]\n",
    "tracking_uri = os.environ['MLFLOWSERVER']\n",
    "mlflow_secret_name = os.environ['MLFLOW_SECRET_NAME']\n",
    "mlflow_key = os.environ['MLFLOW_KEY']\n",
    "experiment_name = 'california-housing'\n",
    "model_name = 'california-housing-model'\n",
    "\n",
    "print('SageMaker role: {}'.format(role.split(\"/\")[-1]))\n",
    "print('bucket: {}'.format(bucket))\n",
    "print('Account: {}'.format(account))\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "print(\"MLflow server URI: {}\".format(tracking_uri))\n",
    "print(\"MLFLOW_SECRET_NAME: {}\").format(mlflow_secret_name)\n",
    "print(\"MLFLOW_KEY: {}\".format(mlflow_key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We load the dataset from sklearn, then split the data in training and testing datasets, where we allocate 75% of the data to the training dataset, and the remaining 25% to the traning dataset.\n",
    "\n",
    "The variable `target` is what we intend to estimate, which represents the value of a house, expressed in hundreds of thousands of dollars ($100,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the California housing dataset \n",
    "data = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "testX['target'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save a copy of the data locally, as well as in S3. The data stored in S3 will be used SageMaker to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data locally\n",
    "trainX.to_csv('california_train.csv', index=False)\n",
    "testX.to_csv('california_test.csv', index=False)\n",
    "\n",
    "# save the data to S3.\n",
    "train_path = sess.upload_data(path='california_train.csv', bucket=bucket, key_prefix='sagemaker/sklearncontainer')\n",
    "test_path = sess.upload_data(path='california_test.csv', bucket=bucket, key_prefix='sagemaker/sklearncontainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For this example, we use the `SKlearn` framework in script mode with SageMaker. Let us explore in more details the different components we need to define.\n",
    "\n",
    "### Traning script\n",
    "\n",
    "The `./source_dir/train.py` script provides all the code we need for training a SageMaker model. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_CHANNEL_TRAIN`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "* `SM_CHANNEL_TEST`: A string representing the path to the directory containing data in the 'testing' channel.\n",
    "\n",
    "For more information about training environment variables, please visit \n",
    "[SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit).\n",
    "\n",
    "#### Hyperparmeters\n",
    "\n",
    "We are using the `RandomForestRegressor` algorithm from the SKlearn framework. For the purpose of this exercise, we are only using a subset of hyperparameters supported by this algorithm, i.e. `n-estimators` and `min-samples-leaf`\n",
    "\n",
    "If you would like to know more the different hyperparmeters for this algorithm, please refer to the [`RandomForestRegressor` official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
    "\n",
    "Furthermore, it is important to note that for the purpose of this excercise, we are essentially omitting completely the feature engineering step, which is an essential step in any machine learning problem.\n",
    "\n",
    "#### MLFlow interaction\n",
    "\n",
    "To interact with the MLFlow server, we use the mlflow SDK, which allows us to set the tracking URI and the experiment name. One this initial setup is completed, we can store the parameters used (`mlflow.log_params(params)`), the model that is generated (`mlflow.sklearn.log_model(model, \"model\")`) with its associated metrics (`mlflow.log_metric(f'AE-at-{str(q)}th-percentile', np.percentile(a=abs_err, q=q))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./source_dir/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKlearn container\n",
    "\n",
    "For this example, we use the `SKlearn` framework in script mode with SageMaker. For more information please refere to [the official documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html)\n",
    "\n",
    "Our training script makes use of other 3rd party libraries, i.e. `mlflow`, which are not installed by default in the `Sklearn` container SageMaker provides. However, this can be easily overcome by supplying a `requirement.txt` file in the `source_dir` folder, which then SageMaker will `pip`-install before executing the training script.\n",
    "\n",
    "### Metric definition\n",
    "\n",
    "SageMaker emits every log to CLoudWatch. Since we are using scripting mode, we need to specify a metric definition object to define the format of the metric we are interested in via regex, so that SageMaker knows how to extract this metric from the CloudWatch logs of the training job.\n",
    "\n",
    "In our case our custom metric is as follow\n",
    "\n",
    "```python\n",
    "metric_definitions = [{'Name': 'median-AE', 'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"}]\n",
    "```\n",
    "\n",
    "### Local mode\n",
    "\n",
    "During early experimentation, and for testing that your script is bug-free, it is a good practise to run SageMaker in `local` mode with a small amount of data. When you specify the `instance_type='local'`, you are instructing the SageMaker SDK to run the training on your local machine rather then in the SageMaker remote managed infrastructure. This approach makes it quicker to address problems while developing your script.\n",
    "\n",
    "Once you are confident that your code is correct, you can then offload the training to the SageMaker managed infrastructure. We will see this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name': 'median-AE', 'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"}]\n",
    "\n",
    "hyperparameters = {\n",
    "    'tracking_uri': tracking_uri,\n",
    "    'experiment_name': experiment_name,\n",
    "    'n-estimators': 100,\n",
    "    'min-samples-leaf': 3,\n",
    "    'features': 'MedInc HouseAge AveRooms AveBedrms Population AveOccup',\n",
    "    'target': 'target'\n",
    "}\n",
    "\n",
    "estimator = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    source_dir='source_dir',\n",
    "    role=role,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type='local',        # to run SageMaker in local mode\n",
    "    framework_version='0.23-1',\n",
    "    base_job_name='mlflow',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to execute the training locally, which in turn will save its execution data to the MLFlow server. After initializing an `SKlearn` estimator object, all we need to do is to call the `.fit` method specifying where the training and testing data are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train':train_path, 'test': test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model to MLFlow\n",
    "\n",
    "At the end of the training, our model has been saved to the MLflow server and we are ready to register the model, i.e. assign it to a model package and create a version. Please refer to the [official MLFlow documentation](https://www.mlflow.org/docs/latest/model-registry.html) for furthe information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "client = MlflowClient()\n",
    "\n",
    "# Find the experiment ID\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Get the latest run\n",
    "run = client.search_runs(\n",
    "  experiment_ids=experiment_id,\n",
    "  filter_string=\"\",\n",
    "  max_results=1,\n",
    "  order_by=[\"attribute.start_time DESC\"]\n",
    ")[0]\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(model_name)\n",
    "except:\n",
    "    print(\"Registered model already exists\")\n",
    "\n",
    "model_version = client.create_model_version(\n",
    "    name=model_name,\n",
    "    source=\"{}/model\".format(run.info.artifact_uri),\n",
    "    run_id=run.info.run_uuid\n",
    ")\n",
    "\n",
    "print(\"model_version: {}\".format(model_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Predictions\n",
    "\n",
    "We are now ready to make predictions with our model locally for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model URI from the MLFlow registry\n",
    "model_uri = model_version.source\n",
    "print(\"Model URI: {}\".format(model_uri))\n",
    "\n",
    "# Load model as a Sklearn model.\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# get a random index to test the prediction from the test data\n",
    "index = random.randrange(0, len(testX))\n",
    "print(\"Random index value: {}\".format(index))\n",
    "\n",
    "# Prepare data on a Pandas DataFrame to make a prediction.\n",
    "data = testX.drop(['Latitude','Longitude','target'], axis=1).iloc[[index]]\n",
    "\n",
    "print(\"#######\\nData for prediction \\n{}\".format(data))\n",
    "\n",
    "y_hat = loaded_model.predict(data)[0]\n",
    "y = y_test[index]\n",
    "\n",
    "print(\"Predicted value: {}\".format(y_hat))\n",
    "print(\"Actual value: {}\".format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune a Scikit-Learn model in SageMaker and track with MLFlow\n",
    "\n",
    "At this point, we are going to offload the training to the remote infrastructure managed by SageMaker. We want now to leverage SageMaker's hyperparameter tuning to kick off multiple training jobs with different hyperparameter combinations, to find the set with best model performance. This is an important step in the machine learning process as hyperparameter settings can have a large impact on model accuracy. In this example, we'll use the SageMaker Python SDK to create a hyperparameter tuning job for an SKlearn estimator.\n",
    "\n",
    "## Training\n",
    "We are again using `SKlearn` in script mode, with the same training script we have used in the previous section, i.e. `./source_dir/train.py`. The only different is that we are specifying an `instance_type` $\\neq$ `local`. Indeed, we are specifying now the instance type we want the training job to run to, in our specific case this is a `ml.m5.xlarge` instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'tracking_uri': tracking_uri,\n",
    "    'experiment_name': experiment_name,\n",
    "    'features': 'MedInc HouseAge AveRooms AveBedrms Population AveOccup',\n",
    "    'target': 'target'\n",
    "}\n",
    "\n",
    "metric_definitions = [{'Name': 'median-AE', 'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"}]\n",
    "\n",
    "estimator = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    source_dir='source_dir',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    framework_version='0.23-1',\n",
    "    py_version='py3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined our estimator we can specify the hyperparameters we'd like to tune and their possible values.  We have three different types of hyperparameters.\n",
    "- Categorical parameters need to take one value from a discrete set.  We define this by passing the list of possible values to `CategoricalParameter(list)`\n",
    "- Continuous parameters can take any real number value between the minimum and maximum value, defined by `ContinuousParameter(min, max)`\n",
    "- Integer parameters can take any integer value between the minimum and maximum value, defined by `IntegerParameter(min, max)`\n",
    "\n",
    "*Note, if possible, it's almost always best to specify a value as the least restrictive type.  For example, tuning `thresh` as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with possible values of 0.01, 0.1, 0.15, or 0.2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'n-estimators': IntegerParameter(50, 200),\n",
    "    'min-samples-leaf': IntegerParameter(1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition. This refers to the regular expression (Regex) needed to extract that metric from the CloudWatch logs of our training job we defined earlier, as well as whether we are looking to `Maximize` or `Minimize` the objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'median-AE'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a `HyperparameterTuner` object, which we pass:\n",
    "- The SKLearn estimator we created earlier\n",
    "- Our hyperparameter ranges\n",
    "- Objective metric name and type\n",
    "- Number of training jobs to run in total and how many training jobs should be run simultaneously.  More parallel jobs will finish tuning sooner, but may sacrifice accuracy.  We recommend you set the parallel jobs value to less than 10% of the total number of training jobs (we'll set it higher just for this example to keep it short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_jobs = 10\n",
    "max_parallel_jobs = 3\n",
    "\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=max_jobs,\n",
    "                            max_parallel_jobs=max_parallel_jobs,\n",
    "                            objective_type=objective_type,\n",
    "                            base_tuning_job_name='mlflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can start our tuning job by calling `.fit()` and passing in the S3 paths to our train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train':train_path, 'test': test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the MLFlow server to see the different models and their metrics that have been stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy an MLflow model with SageMaker\n",
    "\n",
    "We are finally ready to deploy a MLFlow model to a SageMaker hosted endpoint ready to be consumed for online predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MLflow docker image to serve the model with SageMaker\n",
    "\n",
    "We first need to build a new MLflow Sagemaker image, assign it a name, and push to ECR.\n",
    "\n",
    "The `mlflow sagemaker build-and-push-container` function does exactly that. It first builds an MLflow Docker image. The image is built locally and it requires Docker to run. Then, the image is pushed to ECR under current active AWS account and to current active AWS region. More information on this command can be found in the official [MLflow CLI documentation for SageMaker](https://www.mlflow.org/docs/latest/cli.html#mlflow-sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow sagemaker build-and-push-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the ECR-hosted Docker image the model should be deployed into: make sure to include the tag 1.18.0\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/mlflow-pyfunc:1.18.0\".format(account, region)\n",
    "print(\"image URI: {}\".format(image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a SageMaker endpoint with our scikit-learn model\n",
    "\n",
    "We first need to get the best performing model stored in MLFlow. Once it has been identified, we register it to the Registry and then deploy to a SageMaker managed endpoint via the MLflow SDK. More information can be found [here](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Get the best run according to the objective metric\n",
    "run = client.search_runs(\n",
    "  experiment_ids=experiment_id,\n",
    "  filter_string=\"\",\n",
    "  max_results=1,\n",
    "  order_by=[\"metrics.`AE-at-50th-percentile` ASC\"]\n",
    ")[0]\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(model_name)\n",
    "except:\n",
    "    print(\"Registered model already exists\")\n",
    "\n",
    "model_version = client.create_model_version(\n",
    "    name=model_name,\n",
    "    source=\"{}/model\".format(run.info.artifact_uri),\n",
    "    run_id=run.info.run_uuid\n",
    ")\n",
    "\n",
    "print(\"model_version: {}\".format(model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = \"models:/{}/{}\".format(model_version.name, model_version.version)\n",
    "\n",
    "endpoint_name = 'california-housing'\n",
    "\n",
    "mlflow.sagemaker.deploy(\n",
    "    mode='create',\n",
    "    app_name=endpoint_name,\n",
    "    model_uri=model_uri,\n",
    "    image_url=image_uri,\n",
    "    execution_role_arn=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    region_name=region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "We are now ready to make predictions again the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load california  dataset\n",
    "data = pd.read_csv('./california_test.csv')\n",
    "df_y = data[['target']]\n",
    "df = data.drop(['Latitude','Longitude','target'], axis=1)\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "for _ in range(0,2):\n",
    "    # Randomly pick a row to test the prediction\n",
    "    index = random.randrange(0, len(df_y))\n",
    "    payload = df.iloc[[index]].to_json(orient=\"split\")\n",
    "    y = df_y['target'][index]\n",
    "    runtime_response = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=payload)\n",
    "    prediction = json.loads(runtime_response['Body'].read().decode())\n",
    "    #print(f'Payload: {payload}')\n",
    "    print(f'This is the real value of the housing we want to predict (expressed in 100.000$): {y}')\n",
    "    print(f'This is the predicted value from our model (expressed in 100.000$): {prediction[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint\n",
    "\n",
    "In order to avoid unwanted costs, make sure you delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sagemaker.delete(app_name=endpoint_name, region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
